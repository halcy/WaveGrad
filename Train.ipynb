{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdce680-261e-406a-bd1c-c9543d49a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from logger import Logger\n",
    "from model import WaveGrad\n",
    "from data import AudioDataset, MelSpectrogramFixed\n",
    "from benchmark import compute_rtf\n",
    "\n",
    "from utils import ConfigWrapper, show_message, str2bool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f625803c-ae81-4f89-af5a-a3a137857d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/ddpm/WaveGrad\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2496d86e-e12f-4c7a-97be-4a3e5fd56c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(rank, config):\n",
    "    show_message('Initializing logger...', verbose=config.verbose, rank=rank)\n",
    "    logger = Logger(config, rank=rank)\n",
    "    \n",
    "    mel_fn = MelSpectrogramFixed(\n",
    "        sample_rate=config.data_config.sample_rate,\n",
    "        n_fft=config.data_config.n_fft,\n",
    "        win_length=config.data_config.win_length,\n",
    "        hop_length=config.data_config.hop_length,\n",
    "        f_min=config.data_config.f_min,\n",
    "        f_max=config.data_config.f_max,\n",
    "        n_mels=config.data_config.n_mels,\n",
    "        window_fn=torch.hann_window\n",
    "    ).cuda()\n",
    "    \n",
    "    show_message('Initializing model...', verbose=config.verbose, rank=rank)\n",
    "    model = WaveGrad(config).cuda()\n",
    "    show_message(f'Number of WaveGrad parameters: {model.nparams}', verbose=config.verbose, rank=rank)\n",
    "\n",
    "    show_message('Initializing optimizer, scheduler and losses...', verbose=config.verbose, rank=rank)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.training_config.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.training_config.scheduler_step_size,\n",
    "        gamma=config.training_config.scheduler_gamma\n",
    "    )\n",
    "    if config.training_config.use_fp16:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    show_message('Initializing data loaders...', verbose=config.verbose, rank=rank)\n",
    "    train_dataset = AudioDataset(config, training=True)\n",
    "    train_sampler = None\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=config.training_config.batch_size,\n",
    "        sampler=train_sampler, drop_last=True,\n",
    "        num_workers=os.cpu_count() - 2\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        test_dataset = AudioDataset(config, training=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "        test_batch = test_dataset.sample_test_batch(\n",
    "            config.training_config.n_samples_to_test\n",
    "        )\n",
    "\n",
    "    if config.training_config.continue_training:\n",
    "        show_message('Loading latest checkpoint to continue training...', verbose=config.verbose, rank=rank)\n",
    "        model, optimizer, iteration = logger.load_latest_checkpoint(model, optimizer)\n",
    "        epoch_size = len(train_dataset) // config.training_config.batch_size\n",
    "        epoch_start = iteration // epoch_size\n",
    "    else:\n",
    "        iteration = 0\n",
    "        epoch_start = 0\n",
    "\n",
    "    # Log ground truth test batch\n",
    "    if rank == 0:\n",
    "        audios = {\n",
    "            f'audio_{index}/gt': audio\n",
    "            for index, audio in enumerate(test_batch)\n",
    "        }\n",
    "        logger.log_audios(0, audios)\n",
    "        specs = {\n",
    "            f'mel_{index}/gt': mel_fn(audio.cuda()).cpu().squeeze()\n",
    "            for index, audio in enumerate(test_batch)\n",
    "        }\n",
    "        #logger.log_specs(0, specs)\n",
    "\n",
    "    show_message('Start training...', verbose=config.verbose, rank=rank)\n",
    "    try:\n",
    "        for epoch in range(epoch_start, config.training_config.n_epoch):\n",
    "            # Training step\n",
    "            model.train()\n",
    "            model.set_new_noise_schedule(\n",
    "                init=torch.linspace,\n",
    "                init_kwargs={\n",
    "                    'steps': config.training_config.training_noise_schedule.n_iter,\n",
    "                    'start': config.training_config.training_noise_schedule.betas_range[0],\n",
    "                    'end': config.training_config.training_noise_schedule.betas_range[1]\n",
    "                }\n",
    "            )\n",
    "            for batch in (\n",
    "                tqdm(train_dataloader, leave=False) \\\n",
    "                if config.verbose and rank == 0 else train_dataloader\n",
    "            ):\n",
    "                model.zero_grad()\n",
    "\n",
    "                batch = batch.cuda()\n",
    "                mels = mel_fn(batch)\n",
    "                \n",
    "                if config.training_config.use_fp16:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        loss = model.compute_loss(mels, batch)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                else:\n",
    "                    loss = model.compute_loss(mels, batch)\n",
    "                    loss.backward()\n",
    "                \n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    parameters=model.parameters(),\n",
    "                    max_norm=config.training_config.grad_clip_threshold\n",
    "                )\n",
    "\n",
    "                if config.training_config.use_fp16:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                loss_stats = {\n",
    "                    'total_loss': loss.item(),\n",
    "                    'grad_norm': grad_norm.item()\n",
    "                }\n",
    "                logger.log_training(iteration, loss_stats, verbose=False)\n",
    "                \n",
    "                iteration += 1\n",
    "\n",
    "            # Test step after epoch on rank==0 GPU\n",
    "            if epoch % config.training_config.test_interval == 0 and rank == 0:\n",
    "                model.eval()\n",
    "                model.set_new_noise_schedule(\n",
    "                    init=torch.linspace,\n",
    "                    init_kwargs={\n",
    "                        'steps': config.training_config.test_noise_schedule.n_iter,\n",
    "                        'start': config.training_config.test_noise_schedule.betas_range[0],\n",
    "                        'end': config.training_config.test_noise_schedule.betas_range[1]\n",
    "                    }\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    # Calculate test set loss\n",
    "                    test_loss = 0\n",
    "                    for i, batch in enumerate(\n",
    "                        tqdm(test_dataloader) \\\n",
    "                        if config.verbose and rank == 0 else test_dataloader\n",
    "                    ):\n",
    "                        batch = batch.cuda()\n",
    "                        mels = mel_fn(batch)\n",
    "                        test_loss_ = model.compute_loss(mels, batch)\n",
    "                        test_loss += test_loss_\n",
    "                    test_loss /= (i + 1)\n",
    "                    loss_stats = {'total_loss': test_loss.item()}\n",
    "\n",
    "                    # Restore random batch from test dataset\n",
    "                    audios = {}\n",
    "                    specs = {}\n",
    "                    test_l1_loss = 0\n",
    "                    test_l1_spec_loss = 0\n",
    "                    average_rtf = 0\n",
    "\n",
    "                    for index, test_sample in enumerate(test_batch):\n",
    "                        test_sample = test_sample[None].cuda()\n",
    "                        test_mel = mel_fn(test_sample.cuda())\n",
    "\n",
    "                        start = datetime.now()\n",
    "                        y_0_hat = model.forward(\n",
    "                            test_mel, store_intermediate_states=False\n",
    "                        )\n",
    "                        y_0_hat_mel = mel_fn(y_0_hat)\n",
    "                        end = datetime.now()\n",
    "                        generation_time = (end - start).total_seconds()\n",
    "                        average_rtf += compute_rtf(\n",
    "                            y_0_hat, generation_time, config.data_config.sample_rate\n",
    "                        )\n",
    "                        \n",
    "                        test_l1_loss += torch.nn.L1Loss()(y_0_hat, test_sample).item()\n",
    "                        test_l1_spec_loss += torch.nn.L1Loss()(y_0_hat_mel, test_mel).item()\n",
    "\n",
    "                        audios[f'audio_{index}/predicted'] = y_0_hat.cpu().squeeze()\n",
    "                        specs[f'mel_{index}/predicted'] = y_0_hat_mel.cpu().squeeze()\n",
    "\n",
    "                    average_rtf /= len(test_batch)\n",
    "                    show_message(f'Device: GPU. average_rtf={average_rtf}', verbose=config.verbose)\n",
    "\n",
    "                    test_l1_loss /= len(test_batch)\n",
    "                    loss_stats['l1_test_batch_loss'] = test_l1_loss\n",
    "                    test_l1_spec_loss /= len(test_batch)\n",
    "                    loss_stats['l1_spec_test_batch_loss'] = test_l1_spec_loss\n",
    "\n",
    "                    logger.log_test(iteration, loss_stats, verbose=config.verbose)\n",
    "                    logger.log_audios(iteration, audios)\n",
    "                    #logger.log_specs(iteration, specs)\n",
    "\n",
    "                logger.save_checkpoint(\n",
    "                    iteration,\n",
    "                    model,\n",
    "                    optimizer\n",
    "                )\n",
    "            if epoch % (epoch//10 + 1) == 0:\n",
    "                scheduler.step()\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt: training has been stopped.')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba93aed-46fc-4c94-83c4-45049d81f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "file_list = glob.glob(\"../meow/*.wav\")\n",
    "random.shuffle(file_list)\n",
    "\n",
    "\n",
    "with open(\"train.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(sorted(file_list)))\n",
    "    \n",
    "with open(\"test.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(file_list[:10]))\n",
    "\n",
    "config = {\n",
    "    \"model_config\": {\n",
    "        \"factors\": [5, 4, 3, 2, 2],\n",
    "        \"upsampling_preconv_out_channels\": 384,\n",
    "        \"upsampling_out_channels\": [128, 128, 64, 32, 32],\n",
    "        \"upsampling_dilations\": [\n",
    "            [1, 2, 4, 8],\n",
    "            [1, 2, 4, 8],\n",
    "            [1, 2, 4, 8],\n",
    "            [1, 2, 1, 2],\n",
    "            [1, 2, 1, 2],\n",
    "        ],\n",
    "        \"downsampling_preconv_out_channels\": 32,\n",
    "        \"downsampling_out_channels\": [128, 128, 256, 512],\n",
    "        \"downsampling_dilations\": [[1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4]],\n",
    "    },\n",
    "    \"data_config\": {\n",
    "        \"sample_rate\": 8000,\n",
    "        \"n_fft\": 512,\n",
    "        \"win_length\": 512,\n",
    "        \"hop_length\": 240,\n",
    "        \"f_min\": 80.0,\n",
    "        \"f_max\": 8000,\n",
    "        \"n_mels\": 1\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"logdir\": \"logs/default/\" + str(time.time()),\n",
    "        \"continue_training\": False,\n",
    "        \"train_filelist_path\": \"train.txt\",\n",
    "        \"test_filelist_path\": \"test.txt\",\n",
    "        \"batch_size\": 96,\n",
    "        \"segment_length\": 2400,\n",
    "        \"lr\": 0.001,\n",
    "        \"grad_clip_threshold\": 1,\n",
    "        \"scheduler_step_size\": 1,\n",
    "        \"scheduler_gamma\": 0.9,\n",
    "        \"n_epoch\": 100000000,\n",
    "        \"n_samples_to_test\": 4,\n",
    "        \"test_interval\": 500,\n",
    "        \"use_fp16\": True,\n",
    "        \"training_noise_schedule\": {\"n_iter\": 1000, \"betas_range\": [1e-06, 0.01]},\n",
    "        \"test_noise_schedule\": {\"n_iter\": 50, \"betas_range\": [1e-06, 0.01]},\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "}\n",
    "config = ConfigWrapper(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69efae8d-b4ac-4fa1-b0b7-7e84b277cd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing logger...\n",
      "Initializing model...\n",
      "Number of WaveGrad parameters: 5321153\n",
      "Initializing optimizer, scheduler and losses...\n",
      "Initializing data loaders...\n",
      "Loading latest checkpoint to continue training...\n",
      "Latest checkpoint: logs/default/1625170872.8533309\\checkpoint_115005.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-df7c2c3c2715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_gpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d4d9a7543a9a>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(rank, config)\u001b[0m\n\u001b[0;32m     81\u001b[0m             for batch in (\n\u001b[0;32m     82\u001b[0m                 \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             ):\n\u001b[0;32m     85\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda\\envs\\pytorch\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "run_training(0, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf9bee-b165-42a1-b8e5-59d0e2904864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5c556-612c-4f02-96b5-636c885759ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
