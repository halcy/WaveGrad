{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdce680-261e-406a-bd1c-c9543d49a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from logger import Logger\n",
    "from model import WaveGrad\n",
    "from data import AudioDataset, MelSpectrogramFixed\n",
    "from benchmark import compute_rtf\n",
    "\n",
    "from utils import ConfigWrapper, show_message, str2bool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f625803c-ae81-4f89-af5a-a3a137857d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2496d86e-e12f-4c7a-97be-4a3e5fd56c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(rank, config):\n",
    "    show_message('Initializing logger...', verbose=config.verbose, rank=rank)\n",
    "    logger = Logger(config, rank=rank)\n",
    "    \n",
    "    mel_fn = MelSpectrogramFixed(\n",
    "        sample_rate=config.data_config.sample_rate,\n",
    "        n_fft=config.data_config.n_fft,\n",
    "        win_length=config.data_config.win_length,\n",
    "        hop_length=config.data_config.hop_length,\n",
    "        f_min=config.data_config.f_min,\n",
    "        f_max=config.data_config.f_max,\n",
    "        n_mels=config.data_config.n_mels,\n",
    "        window_fn=torch.hann_window\n",
    "    ).cuda()\n",
    "    \n",
    "    show_message('Initializing model...', verbose=config.verbose, rank=rank)\n",
    "    model = WaveGrad(config).cuda()\n",
    "    show_message(f'Number of WaveGrad parameters: {model.nparams}', verbose=config.verbose, rank=rank)\n",
    "\n",
    "    show_message('Initializing optimizer, scheduler and losses...', verbose=config.verbose, rank=rank)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.training_config.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.training_config.scheduler_step_size,\n",
    "        gamma=config.training_config.scheduler_gamma\n",
    "    )\n",
    "    if config.training_config.use_fp16:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    show_message('Initializing data loaders...', verbose=config.verbose, rank=rank)\n",
    "    train_dataset = AudioDataset(config, training=True)\n",
    "    train_sampler = None\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=config.training_config.batch_size,\n",
    "        sampler=train_sampler, drop_last=True\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        test_dataset = AudioDataset(config, training=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "        test_batch = test_dataset.sample_test_batch(\n",
    "            config.training_config.n_samples_to_test\n",
    "        )\n",
    "\n",
    "    if config.training_config.continue_training:\n",
    "        show_message('Loading latest checkpoint to continue training...', verbose=config.verbose, rank=rank)\n",
    "        model, optimizer, iteration = logger.load_latest_checkpoint(model, optimizer)\n",
    "        epoch_size = len(train_dataset) // config.training_config.batch_size\n",
    "        epoch_start = iteration // epoch_size\n",
    "    else:\n",
    "        iteration = 0\n",
    "        epoch_start = 0\n",
    "\n",
    "    # Log ground truth test batch\n",
    "    if rank == 0:\n",
    "        audios = {\n",
    "            f'audio_{index}/gt': audio\n",
    "            for index, audio in enumerate(test_batch)\n",
    "        }\n",
    "        logger.log_audios(0, audios)\n",
    "        specs = {\n",
    "            f'mel_{index}/gt': mel_fn(audio.cuda()).cpu().squeeze()\n",
    "            for index, audio in enumerate(test_batch)\n",
    "        }\n",
    "        #logger.log_specs(0, specs)\n",
    "\n",
    "    show_message('Start training...', verbose=config.verbose, rank=rank)\n",
    "    try:\n",
    "        for epoch in range(epoch_start, config.training_config.n_epoch):\n",
    "            # Training step\n",
    "            model.train()\n",
    "            model.set_new_noise_schedule(\n",
    "                init=torch.linspace,\n",
    "                init_kwargs={\n",
    "                    'steps': config.training_config.training_noise_schedule.n_iter,\n",
    "                    'start': config.training_config.training_noise_schedule.betas_range[0],\n",
    "                    'end': config.training_config.training_noise_schedule.betas_range[1]\n",
    "                }\n",
    "            )\n",
    "            for batch in (\n",
    "                tqdm(train_dataloader, leave=False) \\\n",
    "                if config.verbose and rank == 0 else train_dataloader\n",
    "            ):\n",
    "                model.zero_grad()\n",
    "\n",
    "                batch = batch.cuda()\n",
    "                mels = mel_fn(batch)\n",
    "                \n",
    "                if config.training_config.use_fp16:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        loss = model.compute_loss(mels, batch)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                else:\n",
    "                    loss = model.compute_loss(mels, batch)\n",
    "                    loss.backward()\n",
    "                \n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    parameters=model.parameters(),\n",
    "                    max_norm=config.training_config.grad_clip_threshold\n",
    "                )\n",
    "\n",
    "                if config.training_config.use_fp16:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                loss_stats = {\n",
    "                    'total_loss': loss.item(),\n",
    "                    'grad_norm': grad_norm.item()\n",
    "                }\n",
    "                logger.log_training(iteration, loss_stats, verbose=False)\n",
    "                \n",
    "                iteration += 1\n",
    "\n",
    "            # Test step after epoch on rank==0 GPU\n",
    "            if epoch % config.training_config.test_interval == 0 and rank == 0:\n",
    "                model.eval()\n",
    "                model.set_new_noise_schedule(\n",
    "                    init=torch.linspace,\n",
    "                    init_kwargs={\n",
    "                        'steps': config.training_config.test_noise_schedule.n_iter,\n",
    "                        'start': config.training_config.test_noise_schedule.betas_range[0],\n",
    "                        'end': config.training_config.test_noise_schedule.betas_range[1]\n",
    "                    }\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    # Calculate test set loss\n",
    "                    test_loss = 0\n",
    "                    for i, batch in enumerate(\n",
    "                        tqdm(test_dataloader) \\\n",
    "                        if config.verbose and rank == 0 else test_dataloader\n",
    "                    ):\n",
    "                        batch = batch.cuda()\n",
    "                        mels = mel_fn(batch)\n",
    "                        test_loss_ = model.compute_loss(mels, batch)\n",
    "                        test_loss += test_loss_\n",
    "                    test_loss /= (i + 1)\n",
    "                    loss_stats = {'total_loss': test_loss.item()}\n",
    "\n",
    "                    # Restore random batch from test dataset\n",
    "                    audios = {}\n",
    "                    specs = {}\n",
    "                    test_l1_loss = 0\n",
    "                    test_l1_spec_loss = 0\n",
    "                    average_rtf = 0\n",
    "\n",
    "                    for index, test_sample in enumerate(test_batch):\n",
    "                        test_sample = test_sample[None].cuda()\n",
    "                        test_mel = mel_fn(test_sample.cuda())\n",
    "\n",
    "                        start = datetime.now()\n",
    "                        y_0_hat = model.forward(\n",
    "                            test_mel, store_intermediate_states=False\n",
    "                        )\n",
    "                        y_0_hat_mel = mel_fn(y_0_hat)\n",
    "                        end = datetime.now()\n",
    "                        generation_time = (end - start).total_seconds()\n",
    "                        average_rtf += compute_rtf(\n",
    "                            y_0_hat, generation_time, config.data_config.sample_rate\n",
    "                        )\n",
    "                        \n",
    "                        test_l1_loss += torch.nn.L1Loss()(y_0_hat, test_sample).item()\n",
    "                        test_l1_spec_loss += torch.nn.L1Loss()(y_0_hat_mel, test_mel).item()\n",
    "\n",
    "                        audios[f'audio_{index}/predicted'] = y_0_hat.cpu().squeeze()\n",
    "                        specs[f'mel_{index}/predicted'] = y_0_hat_mel.cpu().squeeze()\n",
    "\n",
    "                    average_rtf /= len(test_batch)\n",
    "                    show_message(f'Device: GPU. average_rtf={average_rtf}', verbose=config.verbose)\n",
    "\n",
    "                    test_l1_loss /= len(test_batch)\n",
    "                    loss_stats['l1_test_batch_loss'] = test_l1_loss\n",
    "                    test_l1_spec_loss /= len(test_batch)\n",
    "                    loss_stats['l1_spec_test_batch_loss'] = test_l1_spec_loss\n",
    "\n",
    "                    logger.log_test(iteration, loss_stats, verbose=config.verbose)\n",
    "                    logger.log_audios(iteration, audios)\n",
    "                    #logger.log_specs(iteration, specs)\n",
    "\n",
    "                logger.save_checkpoint(\n",
    "                    iteration,\n",
    "                    model,\n",
    "                    optimizer\n",
    "                )\n",
    "            if epoch % (epoch//10 + 1) == 0:\n",
    "                scheduler.step()\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt: training has been stopped.')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba93aed-46fc-4c94-83c4-45049d81f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "file_list = glob.glob(\"../meow/*.wav\")\n",
    "random.shuffle(file_list)\n",
    "\n",
    "\n",
    "with open(\"train.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(sorted(file_list)))\n",
    "    \n",
    "with open(\"test.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(file_list[:10]))\n",
    "\n",
    "config = {\n",
    "    \"model_config\": {\n",
    "        \"factors\": [5, 4, 3, 2, 2],\n",
    "        \"upsampling_preconv_out_channels\": 384,\n",
    "        \"upsampling_out_channels\": [128, 128, 64, 32, 32],\n",
    "        \"upsampling_dilations\": [\n",
    "            [1, 2, 4, 8],\n",
    "            [1, 2, 4, 4],\n",
    "            [1, 2, 4, 4],\n",
    "            [1, 2, 1, 2],\n",
    "            [1, 2, 1, 2],\n",
    "        ],\n",
    "        \"downsampling_preconv_out_channels\": 32,\n",
    "        \"downsampling_out_channels\": [128, 128, 256, 512],\n",
    "        \"downsampling_dilations\": [[1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4]],\n",
    "    },\n",
    "    \"data_config\": {\n",
    "        \"sample_rate\": 8000,\n",
    "        \"n_fft\": 512,\n",
    "        \"win_length\": 512,\n",
    "        \"hop_length\": 240,\n",
    "        \"f_min\": 80.0,\n",
    "        \"f_max\": 8000,\n",
    "        \"n_mels\": 1\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"logdir\": \"logs/default/\" + str(time.time()),\n",
    "        \"continue_training\": False,\n",
    "        \"train_filelist_path\": \"train.txt\",\n",
    "        \"test_filelist_path\": \"test.txt\",\n",
    "        \"batch_size\": 96,\n",
    "        \"segment_length\": 2400,\n",
    "        \"lr\": 0.001,\n",
    "        \"grad_clip_threshold\": 1,\n",
    "        \"scheduler_step_size\": 1,\n",
    "        \"scheduler_gamma\": 0.9,\n",
    "        \"n_epoch\": 100000000,\n",
    "        \"n_samples_to_test\": 4,\n",
    "        \"test_interval\": 500,\n",
    "        \"use_fp16\": True,\n",
    "        \"training_noise_schedule\": {\"n_iter\": 1000, \"betas_range\": [1e-06, 0.01]},\n",
    "        \"test_noise_schedule\": {\"n_iter\": 50, \"betas_range\": [1e-06, 0.01]},\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "}\n",
    "config = ConfigWrapper(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efae8d-b4ac-4fa1-b0b7-7e84b277cd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing logger...\n",
      "Initializing model...\n",
      "Number of WaveGrad parameters: 5321153\n",
      "Initializing optimizer, scheduler and losses...\n",
      "Initializing data loaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:100: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: GPU. average_rtf=0.3469790084565412\n",
      "Iteration: 5 | Losses: [5.617231369018555, 0.9868875294923782, 8.078983187675476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:03<00:02,  1.21s/it]"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "run_training(0, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf9bee-b165-42a1-b8e5-59d0e2904864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5c556-612c-4f02-96b5-636c885759ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
